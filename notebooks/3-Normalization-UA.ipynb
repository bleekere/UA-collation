{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "## Case and whitespace\n",
    "\n",
    "We'll start with an easy one: normalising cases. First, let's see the collation output without normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import*\n",
    "collation_object = Collation()\n",
    "collation_object.add_plain_witness('W1', 'The Queen of England & the Prince of Wales.')\n",
    "collation_object.add_plain_witness('W2', 'the queen of england and the prince of wales!')\n",
    "result = collate(collation_object, output='svg_simple', segmentation=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tokens `Queen` in witness 1 and `queen` in witness 2 are *not* considered matches. \n",
    "\n",
    "### 1. Manual normalization:\n",
    "Let's say we'd like the alignment algorithm to ignore the cases of the tokens, so that `queen` will be matched with `Queen`. You can of course normalize the original witnesses. However, CollateX also has a special funtionality for normalization: for each token, CollateX creates two properties: a `t` property for the original token and an `n` property for a normalized form of the original token. If you don't supply an `n` property, CollateX automatically copies the value of the `t` property in the `n` property.\n",
    "\n",
    "You can think of the `n` property as a _shadow_ copy of the token: the user will typically not get to see it, but the aligner will only look at the normalized form during the alignment process.\n",
    "\n",
    "The original token `t` and the normalized form `n` are expressed in JSON. You can put whatever you want in the `n` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "import json\n",
    "norm_collation_obj = Collation()\n",
    "json_input = \"\"\"{\n",
    "\"witnesses\": [\n",
    "{\"id\": \"W1\", \"tokens\": [\n",
    "{\"t\": \"The\", \"n\": \"the\"},\n",
    "{\"t\": \"Queen\", \"n\": \"queen\"},\n",
    "{\"t\":\"of\", \"n\": \"of\"}, \n",
    "{\"t\":\"England\", \"n\": \"england\"},\n",
    "{\"t\":\"&\", \"n\":\"and\"},\n",
    "{\"t\":\"the\", \"n\": \"the\"},\n",
    "{\"t\":\"Prince\", \"n\": \"prince\"},\n",
    "{\"t\":\"of\", \"n\": \"of\"},\n",
    "{\"t\":\"Wales\", \"n\": \"wales\"},\n",
    "{\"t\":\".\", \"n\": \"!\"}\n",
    "]\n",
    "},\n",
    "{\"id\": \"W2\", \"tokens\": [\n",
    "{\"t\": \"the\", \"n\": \"the\"},\n",
    "{\"t\": \"queen\", \"n\": \"queen\"},\n",
    "{\"t\":\"of\", \"n\": \"of\"}, \n",
    "{\"t\":\"england\", \"n\": \"england\"},\n",
    "{\"t\":\"and\", \"n\":\"and\"},\n",
    "{\"t\":\"the\", \"n\": \"the\"},\n",
    "{\"t\":\"prince\", \"n\": \"prince\"},\n",
    "{\"t\":\"of\", \"n\": \"of\"},\n",
    "{\"t\":\"wales\", \"n\": \"wales\"},\n",
    "{\"t\":\"!\", \"n\": \"!\"}\n",
    "]\n",
    "}\n",
    "]\n",
    "}\"\"\"\n",
    "collate(json.loads(json_input), segmentation=False, output='html2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this manual entry of shadow tokens in JSON is time-consuming and doesn't scale for a typical project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess the witnesses\n",
    "We can also preprocess the witnesses to create the dictionary structure and feed that to CollateX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collatex import *\n",
    "\n",
    "#create the witnesses:\n",
    "wA = \"A\", \"The Queen of England, and the Prince (of Wales).\"\n",
    "wB = \"B\", \"the queen of england with the prince of wales!\"\n",
    "\n",
    "collation = Collation()\n",
    "def processToken(inputText): \n",
    "    return {\"t\": inputText, \"n\": re.sub('\\s+$', '', inputText.lower())}\n",
    "def processWitness(inputText):\n",
    "    siglum, rdg = inputText\n",
    "    return {\"id\": siglum, \"tokens\": \\\n",
    "           [processToken(token) for token in re.findall(r'\\w+\\s*|\\W+', rdg)]}\n",
    "witnesses = [wA,wB]\n",
    "collation = {\"witnesses\": [processWitness(witness) for witness in witnesses]}\n",
    "output = collate(collation, segmentation=False, output=\"svg_simple\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitespace\n",
    "\n",
    "Who can tell with which function we have taken care of any trailing whitespace in the input witnesses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuaction normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some researchers are not interested in variation that only affects punctuation. Use the code above and change the normalization so that the witnesses are aligned on words only, and punctuation is ignored.  \n",
    "\n",
    "Hint: use regular expressions to strip the punctuation from the \"n\" properties of the tokens. You can find a regex cheat sheet [here](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf) and practice [here](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "If you're interested in the theoretical side of this, I can highly recommend [a recent article](http://www.digitalhumanities.org/dhq/vol/14/3/000489/000489.html) by David J. Birnbaum and Elena Spadini on the role of normalization in the collation process.\n",
    "\n",
    "They state:\n",
    "\n",
    "### \"Normalization, in a nutshell, makes it possible to identify phenomena on multiple orthographic and linguistic levels and use them to create surrogates for the literal word tokens that then shape and interpret the results of collation, tacitly neutralizing and ignoring other distinctions that are present in the literal tokens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
