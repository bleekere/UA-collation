{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we dive deeper into the tokenization of witnesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the Gothenburg model of automated collation?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the default (built-in) settings of CollateX?  \n",
    "- Input witnesses are **tokenized** on whitespace. This is the blank space punctuation in text, here represented as `_`;\n",
    "- Punctuation marks are considered their own tokens;\n",
    "- For each token, trailing whitespace is **normalized** and thus ignored by the aligner; \n",
    "- Uppercase and lowercase letters are considered different.\n",
    "\n",
    "This means, for example, that the tokens `cat` and `cat_` are considered as equal (= a match) by the alignment algortihm.   \n",
    "\n",
    "So far, that's relatively simple. But you may want to consider additional normalization options, for example:\n",
    "- **Lower and upper case**: e.g., `Cat` will be matched with `cat`;\n",
    "- **Graphemes**: e.g, the Latin alphabet regular `s` will be matched with the long `Å¿`;\n",
    "- **Orthographic variation**: e.g., the British spelling `honour` will be matched with the American `honor`;\n",
    "- **Morphological variation**: e.g., inflected forms of a verb, like `is` and `are` will be considered a match;\n",
    "- **Lexical variation**: e.g., `journals` will be aligned with `magazines`.\n",
    "\n",
    "The choices you make here depend on the purpose of your collation. The choices may even vary in different stages of the same project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default settings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, CollateX consideres punctuation marks as separate tokens. This means that the input `Elli's dog` will be tokenized as four tokens: `Elli`, `'`, `s`, and '`dog`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+-------+---+\n",
      "| A | Elli | ' | s | dog   | . |\n",
      "| B | Elli | ' | s | puppy | . |\n",
      "+---+------+---+---+-------+---+\n"
     ]
    }
   ],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "collation.add_plain_witness(\"A\", \"Elli's dog.\")\n",
    "collation.add_plain_witness(\"B\", \"Elli's puppy.\")\n",
    "table = collate(collation, segmentation=False)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, this may not be what you want. For example, when the apostrophe is not a possessive but a contraction, as with ``Wouter doesn't have a dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+-----+---+------+---+-----+---+\n",
      "| A | Wouter | doesn | '   | t | have | a | dog | . |\n",
      "| B | Wouter | does  | not | - | have | a | dog | . |\n",
      "+---+--------+-------+-----+---+------+---+-----+---+\n"
     ]
    }
   ],
   "source": [
    "collation = Collation()\n",
    "collation.add_plain_witness(\"A\", \"Wouter doesn't have a dog.\")\n",
    "collation.add_plain_witness(\"B\", \"Wouter does not have a dog.\")\n",
    "table = collate(collation, segmentation=False)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such cases, you may want to override the default settings of CollateX. Let's take a closer look how we can do that.\n",
    "\n",
    "## Doing your own tokenization\n",
    "Building upon the example above, let's create a tokenization where we \n",
    "1. split a string into tokens on white space;\n",
    "2. create separat tokens for final punctuation (but not internal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Splitting on white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Wouter's dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Wouter's\", 'dog.']\n"
     ]
    }
   ],
   "source": [
    "# import regular expressions\n",
    "import re\n",
    "words = re.split(r'\\s+', input)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have tokenized on whitespace only. Let's make sure that final punctuation marks are treated as a separate token, without splitting on internal punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Wouter's\"], ['dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [re.findall(r'.+\\w|\\W+$', word) for word in words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this regular expression we have defined a token as either a string of any characters that ends in a word character or a string of non-word characters. As a result, `Wouter's` is considered as one token, because it ends with an \"s\" (a word character) and \"dog.\" is separated into two tokens because it doesn't end on a word character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who can tell me in what type of data structure the variable `tokens` is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to unpack that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Wouter's\", 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input = \"Wouter's dog.\"\n",
    "words = re.split(r'\\s+', input)\n",
    "tokens_by_word = [re.findall(r'.+\\w|\\W+$', word) for word in words]\n",
    "tokens = []\n",
    "for item in tokens_by_word:\n",
    "    tokens.extend(item)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the input for CollateX\n",
    "\n",
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CollateX expects our custom tokenized witnesses not as lists of strings, but in the form of a Python dictionary:\n",
    "\n",
    "`{\"witnesses\" : [ witness_a, witness_b ] }`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each witness, in turn, is also a Python dictionary with exactly two properties:  \n",
    "\n",
    "`witness_a = { \"id\" : \"A\", \"tokens\" : tokens_a }`  \n",
    "\n",
    "The keys of the properties are \"id\" and \"tokens\"; the values of the properties are the siglim of the witness (here the string \"A\") and a list of tokens to be aligned (here referred to with the variable `tokens_a`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token in a witness is also a dictionary with at least one property: \"t\" (for \"text\"). So a token for the string \"dog\" would look as follows:  \n",
    "`{ \"t\" : \"dog\" }`  \n",
    "\n",
    "The list of tokens for our example witness \"Wouter's dog\" would look as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating our tokenized witnesses\n",
    "Remember we had a list of tokens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Wouter's\", 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the list into a list of dictionaries that CollateX requires as input for each witness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'t': \"Wouter's\"}, {'t': 'dog'}, {'t': '.'}]\n"
     ]
    }
   ],
   "source": [
    "token_list = [{\"t\" : token} for token in tokens]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's create a function for this tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+---+\n",
      "| A | Wouter's | dog | . |\n",
      "| B | Wouter's | cat | . |\n",
      "+---+----------+-----+---+\n"
     ]
    }
   ],
   "source": [
    "from collatex import *\n",
    "import re\n",
    "\n",
    "def tokenize(input):\n",
    "    words = re.split(r'\\s+', input) # split input on whitespace\n",
    "    tokens_by_word = [re.findall(r'.+\\w|\\W$', word) for word in words] # split on final punctuation\n",
    "    tokens = []\n",
    "    for item in tokens_by_word:\n",
    "        tokens.extend(item)\n",
    "    token_list = [{\"t\" : token} for token in tokens] # create dictionaries for each token\n",
    "    return token_list\n",
    "\n",
    "input_a = \"Wouter's dog.\"\n",
    "input_b = \"Wouter's cat.\"\n",
    "\n",
    "tokens_a = tokenize(input_a)\n",
    "tokens_b = tokenize(input_b)\n",
    "witness_a = {\"id\" : \"A\", \"tokens\" : tokens_a }\n",
    "witness_b = {\"id\": \"B\", \"tokens\" : tokens_b }\n",
    "input = {\"witnesses\" : [ witness_a, witness_b ]}\n",
    "table = collate(input, segmentation=False)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
